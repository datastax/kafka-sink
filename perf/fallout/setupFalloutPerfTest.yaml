# branch and version of the kafka-connector to benchmark
branch: "1.x"
version: "1.2.1-SNAPSHOT"

# connector type to test, could be json or avro
connector_type: "json"
---

ensemble:
  server:
    node.count: 5
    provisioner:
      name: ctool
      properties:
        cloud.provider: openstack
        cloud.tenant: performance
        cloud.instance.type: ms1.small
        name: dseserver-kafka-perf-json
        mark_for_reuse: true
    configuration_manager:
      - name: ctool
        properties:
          java.version: "1.8_151"
          product.type: dse
          product.install.type: tarball
          product.version: 6.7-dev
          enable.graph: false
          json.topology: |
            {
               "cluster":
                {
                  "snitch":"GossipingPropertyFileSnitch",
                  "nodes":
                  {
                    "0":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra", "seed":"True"},
                    "1":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra", "seed":"True"},
                    "2":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra"},
                    "3":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra"},
                    "4":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra"}
                  }
                }
            }
      - name: ctool_monitoring
        properties:
          components: os,jvm, dse-db, cassandra-all
  observer:
    node.count: 1
    provisioner:
      name: ctool
      properties:
        cloud.tenant: performance
        cloud.instance.type: m1.xlarge
    configuration_manager:
      - name: ctool_monitoring
        properties:
          graphite.create_server: true
  clients:
    - name: kafkabrokers
      node.count: 3
      provisioner:
        name: ctool
        properties:
          cloud.provider: openstack
          cloud.tenant: performance
          cloud.instance.type: ms1.small
          mark_for_reuse: true
      configuration_manager:
        - name: ctool
          properties:
            install.maven: true
            java.version: shenandoah8
        - name: ctool_monitoring
          properties:
            components: os

    - name: kafkaconnect
      node.count: 3
      provisioner:
        name: ctool
        properties:
          cloud.provider: openstack
          cloud.tenant: performance
          cloud.instance.type: ms1.small
          mark_for_reuse: true
      configuration_manager:
        - name: ctool
          properties:
            install.maven: true
            java.version: shenandoah8
        - name: ctool_monitoring
          properties:
            components: os

workload:
  phases:
    - downoad-confluent:
        module: bash
        properties:
          target.group: kafkabrokers
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            curl -O http://packages.confluent.io/archive/5.2/confluent-community-5.2.1-2.12.tar.gz
            mkdir confluent; tar xzf confluent-community-5.2.1-2.12.tar.gz -C confluent --strip-components=1
    - set-server-properties-0:
        module: bash
        properties:
          target.group: kafkabrokers
          target.ordinals: 0
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            sed -i \"s/broker.id=.*/broker.id=0/\" confluent/etc/kafka/server.properties
            mkdir /tmp/zookeeper/ -p; touch /tmp/zookeeper/myid; echo 0 >> /tmp/zookeeper/myid
    - set-server-properties-1:
        module: bash
        properties:
          target.group: kafkabrokers
          target.ordinals: 1
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            sed -i \"s/broker.id=.*/broker.id=1/\" confluent/etc/kafka/server.properties
            mkdir /tmp/zookeeper/ -p; touch /tmp/zookeeper/myid; echo 1 >> /tmp/zookeeper/myid
    - set-server-properties-2:
        module: bash
        properties:
          target.group: kafkabrokers
          target.ordinals: 2
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            sed -i \"s/broker.id=.*/broker.id=2/\" confluent/etc/kafka/server.properties
            mkdir /tmp/zookeeper/ -p; touch /tmp/zookeeper/myid; echo 1 >> /tmp/zookeeper/myid
    - set-properties-all-brokers:
        module: bash
        properties:
          target.group: kafkabrokers
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            BROKER_FIRST_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 1
            BROKER_SECOND_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 2
            BROKER_THIRD_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 3
            # set broker ips in kafka server props
            sed -i \"s/zookeeper.connect=.*/zookeeper.connect=$BROKER_FIRST_ADDRESS:2181,$BROKER_SECOND_ADDRESS:2181,$BROKER_THIRD_ADDRESS:2181/\" confluent/etc/kafka/server.properties
            # disable confluent metrics
            sed -i \"s/confluent.support.metrics.enable=.*/confluent.support.metrics.enable=false/\" confluent/etc/kafka/server.properties
            # initLimit is timeouts ZooKeeper uses to limit the length of time the ZooKeeper servers in quorum have to connect to a leader
            echo \"initLimit=5\" >> confluent/etc/kafka/zookeeper.properties
            # syncLimit limits how far out of date a server can be from a leader
            echo \"syncLimit=2\" >> confluent/etc/kafka/zookeeper.properties
            # set ips of zookeeper nodes
            echo \"server.0=$BROKER_FIRST_ADDRESS:2888:3888\" >> confluent/etc/kafka/zookeeper.properties
            echo \"server.1=$BROKER_SECOND_ADDRESS:2888:3888\" >> confluent/etc/kafka/zookeeper.properties
            echo \"server.2=$BROKER_THIRD_ADDRESS:2888:3888\" >> confluent/etc/kafka/zookeeper.properties
            # start zookeeper, kafka on all brokers, and schema registry
            confluent/bin/zookeeper-server-start confluent/etc/kafka/zookeeper.properties &> zookeeper.log &
            confluent/bin/kafka-server-start confluent/etc/kafka/server.properties &> kafka.log &
            echo \"kafkastore.bootstrap.servers=PLAINTEXT://localhost:9092\" >> confluent/etc/schema-registry/schema-registry.properties
            ./confluent/bin/schema-registry-start confluent/etc/schema-registry/schema-registry.properties &> schema-registry.log &
            sudo apt-get install -y maven
    - create-topics-brokers-0:
        module: bash
        properties:
          target.group: kafkabrokers
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            confluent/bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 2 --partitions 100 --topic json-stream --config retention.ms=-1 delete.topic.enable=true
            confluent/bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 2 --partitions 100 --topic avro-stream --config retention.ms=-1 delete.topic.enable=true
            git clone https://github.com/datastax/kafka-examples.git
    - setup-confluent-workers:
        module: bash
        properties:
          target.group: kafkaconnect
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            curl -O http://packages.confluent.io/archive/5.2/confluent-community-5.2.1-2.12.tar.gz
            mkdir confluent; tar xzf confluent-community-5.2.1-2.12.tar.gz -C confluent --strip-components=1
            # Setup worker Distributed Properties
            echo \"confluent.support.metrics.enable=false\" >> confluent/etc/kafka/connect-distributed.properties

            # set optimal max.poll depending on the connector type
            if [ "{{connector_type}}" = "json" ]
            then
              echo \"consumer.max.poll.records=500\" >> confluent/etc/kafka/connect-distributed.properties
            elif [ "{{connector_type}}" = "avro" ]
            then
              echo \"consumer.max.poll.records=100\" >> confluent/etc/kafka/connect-distributed.properties
            fi

            sed -i \"s/^group.id=.*/group.id=kc-connect-s-group/\" confluent/etc/kafka/connect-distributed.properties
            BROKER_FIRST_ADDRESS=echo ${FALLOUT_kafkabrokers_PUBLIC_IPS} | cut -d "," -f 1
            BROKER_SECOND_ADDRESS=echo ${FALLOUT_kafkabrokers_PUBLIC_IPS} | cut -d "," -f 2
            BROKER_THIRD_ADDRESS=echo ${FALLOUT_kafkabrokers_PUBLIC_IPS} | cut -d "," -f 3
            sed -i \"s/^bootstrap.servers=.*/bootstrap.servers=$BROKER_FIRST_ADDRESS:9092,$BROKER_SECOND_ADDRESS:9092,$BROKER_THIRD_ADDRESS:9092/\" confluent/etc/kafka/connect-distributed.properties
            curl -O http://central.maven.org/maven2/org/jmxtrans/agent/jmxtrans-agent/1.2.6/jmxtrans-agent-1.2.6.jar

            # clone and build connector
            git clone -b {{branch}} https://github.com/riptano/kafka-sink
            mvn clean install -DskipTests=true -Dmaven.javadoc.skip=true -B -V
            CONNECTOR_JAR_LOCATION=${FALLOUT_SCRATCH_DIR}/kafka-sink/dist/target/kafka-connect-dse-{{version}}.jar
            plugin_path=${FALLOUT_SCRATCH_DIR}/confluent/share,${CONNECTOR_JAR_LOCATION}
            sed -i \"s#plugin\.path.*#plugin\.path=$plugin_path\n#\" confluent/etc/kafka/connect-distributed.properties
    - setup-metrics-worker-0:
        module: bash
        properties:
          target.group: kafkaconnect
          target.ordinals: 1
          export_output: false
          script: |
            CONNECT_FIRST_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 1
            METRICS_PATH=${FALLOUT_SCRATCH_DIR}/kafka-sink/perf/kafka/
            sed -i '72i KAFKA_JMX_OPTS=\"-javaagent:/${FALLOUT_SCRATCH_DIR}/jmxtrans-agent-1.2.6.jar=${METRICS_PATH}kafka-connect-metrics-0.xml -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=$CONNECT_FIRST_ADDRESS -Dcom.sun.management.jmxremote.port=7199\"' confluent/bin/connect-distributed
    - setup-metrics-worker-1:
        module: bash
        properties:
          target.group: kafkaconnect
          target.ordinals: 2
          export_output: false
          script: |
            CONNECT_SECOND_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 2
            METRICS_PATH=${FALLOUT_SCRATCH_DIR}/kafka-sink/perf/kafka/
            sed -i '72i KAFKA_JMX_OPTS=\"-javaagent:/${FALLOUT_SCRATCH_DIR}/jmxtrans-agent-1.2.6.jar=${METRICS_PATH}kafka-connect-metrics-1.xml -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=$CONNECT_SECOND_ADDRESS -Dcom.sun.management.jmxremote.port=7199\"' confluent/bin/connect-distributed
    - setup-metrics-worker-2:
        module: bash
        properties:
          target.group: kafkaconnect
          target.ordinals: 3
          export_output: false
          script: |
            CONNECT_THIRD_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 3
            METRICS_PATH=${FALLOUT_SCRATCH_DIR}/kafka-sink/perf/kafka/
            sed -i '72i KAFKA_JMX_OPTS=\"-javaagent:/${FALLOUT_SCRATCH_DIR}/jmxtrans-agent-1.2.6.jar=${METRICS_PATH}kafka-connect-metrics-2.xml -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=$CONNECT_THIRD_ADDRESS -Dcom.sun.management.jmxremote.port=7199\"' confluent/bin/connect-distributed
    - start-connector:
        module: bash
        properties:
          target.group: kafkaconnect
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            if [ "{{connector_type}}" = "json" ]
            then
                echo "configure for JSON"
                sed -i \"s/^key.converter=.*/key.converter=org.apache.kafka.connect.storage.StringConverter/\" confluent/etc/kafka/connect-distributed.properties
                sed -i \"s/^key.converter.schemas.enable=.*/key.converter.schemas.enable=false/\" confluent/etc/kafka/connect-distributed.properties
                sed -i \"s/^value.converter.schemas.enable=.*/value.converter.schemas.enable=false/\" confluent/etc/kafka/connect-distributed.properties
            elif [ "{{connector_type}}" = "avro" ]
            then
                echo "configure for AVRO"
                BROKER_FIRST_ADDRESS=echo ${FALLOUT_kafkabrokers_PUBLIC_IPS} | cut -d "," -f 1
                BROKER_SECOND_ADDRESS=echo ${FALLOUT_kafkabrokers_PUBLIC_IPS} | cut -d "," -f 2
                BROKER_THIRD_ADDRESS=echo ${FALLOUT_kafkabrokers_PUBLIC_IPS} | cut -d "," -f 3
                sed -i \"s/^key.converter=.*/key.converter=io.confluent.connect.avro.AvroConverter/\" confluent/etc/kafka/connect-distributed.properties
                sed -i \"s/^value.converter=.*/value.converter=io.confluent.connect.avro.AvroConverter/\" confluent/etc/kafka/connect-distributed.properties
                echo \"key.converter.schema.registry.url=http://$BROKER_FIRST_ADDRESS:8081,http://$BROKER_SECOND_ADDRESS:8081,http://$BROKER_THIRD_ADDRESS:8081\" >> confluent/etc/kafka/connect-distributed.properties
                echo \"value.converter.schema.registry.url=http://$BROKER_FIRST_ADDRESS:8081,http://$BROKER_SECOND_ADDRESS:8081,http://$BROKER_THIRD_ADDRESS:8081\" >> confluent/etc/kafka/connect-distributed.properties
                sed -i \"s/^key.converter.schemas.enable=.*/key.converter.schemas.enable=true/\" confluent/etc/kafka/connect-distributed.properties
                sed -i \"s/^value.converter.schemas.enable=.*/value.converter.schemas.enable=true/\" confluent/etc/kafka/connect-distributed.properties
            fi
            confluent/bin/connect-distributed confluent/etc/kafka/connect-distributed.properties &> worker.log &
    - setup-dse-schema:
        module: cqlsh
        properties:
          num.nodes: 1
          command: >
                    CREATE KEYSPACE stocks WITH replication = {'class': 'NetworkTopologyStrategy', 'kc-dc': 3};
                    CREATE TABLE stocks.ticks (symbol text, ts timestamp, exchange text, industry text, name text, value double, PRIMARY KEY (symbol, ts));

                    create keyspace if not exists kafka_examples with replication = {'class': 'NetworkTopologyStrategy', 'kc-dc': 3};

                    CREATE type if not exists kafka_examples.segment0_udt (
                    segment0_0 text,
                    segment0_1 text,
                    segment0_2 text,
                    segment0_3 text,
                    segment0_4 text,
                    segment0_5 text,
                    segment0_6 text,
                    segment0_7 text,
                    segment0_8 text,
                    segment0_9 text
                    );

                    CREATE type if not exists kafka_examples.segment1_udt (
                    segment1_0 text,
                    segment1_1 text,
                    segment1_2 text,
                    segment1_3 text,
                    segment1_4 text,
                    segment1_5 text,
                    segment1_6 text,
                    segment1_7 text,
                    segment1_8 text,
                    segment1_9 text
                    );

                    CREATE type if not exists kafka_examples.segment2_udt (
                    segment2_0 text,
                    segment2_1 text,
                    segment2_2 text,
                    segment2_3 text,
                    segment2_4 text,
                    segment2_5 text,
                    segment2_6 text,
                    segment2_7 text,
                    segment2_8 text,
                    segment2_9 text
                    );

                    CREATE type if not exists kafka_examples.segment3_udt (
                    segment3_0 text,
                    segment3_1 text,
                    segment3_2 text,
                    segment3_3 text,
                    segment3_4 text,
                    segment3_5 text,
                    segment3_6 text,
                    segment3_7 text,
                    segment3_8 text,
                    segment3_9 text
                    );

                    CREATE type if not exists kafka_examples.segment4_udt (
                    segment4_0 text,
                    segment4_1 text,
                    segment4_2 text,
                    segment4_3 text,
                    segment4_4 text,
                    segment4_5 text,
                    segment4_6 text,
                    segment4_7 text,
                    segment4_8 text,
                    segment4_9 text
                    );

                    CREATE type if not exists kafka_examples.segment5_udt (
                    segment5_0 text,
                    segment5_1 text,
                    segment5_2 text,
                    segment5_3 text,
                    segment5_4 text,
                    segment5_5 text,
                    segment5_6 text,
                    segment5_7 text,
                    segment5_8 text,
                    segment5_9 text
                    );
                    CREATE type if not exists kafka_examples.segment6_udt (
                    segment6_0 text,
                    segment6_1 text,
                    segment6_2 text,
                    segment6_3 text,
                    segment6_4 text,
                    segment6_5 text,
                    segment6_6 text,
                    segment6_7 text,
                    segment6_8 text,
                    segment6_9 text
                    );
                    CREATE type if not exists kafka_examples.segment7_udt (
                    segment7_0 text,
                    segment7_1 text,
                    segment7_2 text,
                    segment7_3 text,
                    segment7_4 text,
                    segment7_5 text,
                    segment7_6 text,
                    segment7_7 text,
                    segment7_8 text,
                    segment7_9 text
                    );
                    CREATE type if not exists kafka_examples.segment8_udt (
                    segment8_0 text,
                    segment8_1 text,
                    segment8_2 text,
                    segment8_3 text,
                    segment8_4 text,
                    segment8_5 text,
                    segment8_6 text,
                    segment8_7 text,
                    segment8_8 text,
                    segment8_9 text
                    );

                    CREATE type if not exists kafka_examples.segment9_udt (
                    segment9_0 text,
                    segment9_1 text,
                    segment9_2 text,
                    segment9_3 text,
                    segment9_4 text,
                    segment9_5 text,
                    segment9_6 text,
                    segment9_7 text,
                    segment9_8 text,
                    segment9_9 text
                    );

                    create table if not exists kafka_examples.avro_udt_table (
                    id int PRIMARY KEY,
                    udt_col0 FROZEN<segment0_udt>,
                    udt_col1 FROZEN<segment1_udt>,
                    udt_col2 FROZEN<segment2_udt>,
                    udt_col3 FROZEN<segment3_udt>,
                    udt_col4 FROZEN<segment4_udt>,
                    udt_col5 FROZEN<segment5_udt>,
                    udt_col6 FROZEN<segment6_udt>,
                    udt_col7 FROZEN<segment7_udt>,
                    udt_col8 FROZEN<segment8_udt>,
                    udt_col9 FROZEN<segment9_udt>
                    );
    - start-producer-and-submit-connector-worker-0:
        module: bash
        properties:
          target.group: kafkabrokers
          target.ordinals: 1
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}

            if [ "{{connector_type}}" = "json" ]
            then
              BROKER_FIRST_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 1
              BROKER_SECOND_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 2
              BROKER_THIRD_ADDRESS=echo ${FALLOUT_TARGET_NODES_PUBLIC_IPS} | cut -d "," -f 3
              # Produce 20_000 records per second to json-stream topic
              KC_BROKERS_LIST=${BROKER_FIRST_ADDRESS}:9092,${BROKER_SECOND_ADDRESS}:9092,${BROKER_THIRD_ADDRESS}:9092
              cd kafka-examples/producers; mvn clean compile exec:java -Dexec.mainClass=json.InfiniteJsonProducer -Dexec.args=\"20000 json-stream $KC_BROKERS_LIST\" &> infinite-json-producer.log &

              CONNECT_FIRST_ADDRESS=echo ${FALLOUT_kafkaconnect_PUBLIC_IPS} | cut -d "," -f 1

              DSE_FIRST_ADDRESS=echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 1
              DSE_SECOND_ADDRESS=echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 2

              sed -i "s/{dse_contact_point_1}/$DSE_FIRST_ADDRESS/g" kafka-sink/perf/dse-sink.json
              sed -i "s/{dse_contact_point_2}/$DSE_SECOND_ADDRESS/g" kafka-sink/perf/dse-sink.json
              curl -X POST -H "Content-Type: application/json" -d @kafka-sink/perf/dse-sink.json "$CONNECT_FIRST_ADDRESS:8083/connectors"

            elif [ "{{connector_type}}" = "avro" ]
            then
              # Produce 200_000_000 avro records
              cd kafka-examples/producers; mvn clean compile exec:java -Dexec.mainClass=avro.AvroProducer -Dexec.args=\"avro-stream 200000000\"

              CONNECT_FIRST_ADDRESS=echo ${FALLOUT_kafkaconnect_PUBLIC_IPS} | cut -d "," -f 1

              DSE_FIRST_ADDRESS=echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 1
              DSE_SECOND_ADDRESS=echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 2

              sed -i "s/{dse_contact_point_1}/$DSE_FIRST_ADDRESS/g" kafka-sink/perf/dse-sink-avro.json
              sed -i "s/{dse_contact_point_2}/$DSE_SECOND_ADDRESS/g" kafka-sink/perf/dse-sink-avro.json

              curl -X POST -H "Content-Type: application/json" -d @kafka-sink/perf/dse-sink-avro.json "$CONNECT_FIRST_ADDRESS:8083/connectors"

            fi

  checkers:
    verify_success:
      checker: nofail
